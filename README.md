# Rag Meter

RAGMeter is a universal evaluation toolkit designed to assess the performance of any Retrieval-Augmented Generation (RAG) system

## Usage

To evaluate your RAG model's performance using the `rag_meter` package, follow the steps below.

### Prerequisites

Ensure that you have the following setup:

1. **Install the package**:

   To install the `rag_meter` package, you can use `pip`:

   ```bash
   pip install rag-meter
   ```

2. **Set up your OpenAI API Key**:
   You'll need to provide your OpenAI API key for evaluation. Set it as an environment variable:

   ```bash
   export OPENAI_API_KEY="your-openai-api-key-here"
   ```

   Alternatively, you can set it within your script as shown in the example below.

### Example Usage

The following example demonstrates how to use the `RAGMeter` for evaluating a set of questions, ground truth, and retrieved context.

```python
import asyncio
from rag_meter.evaluation import RAGMeter

# Define an async function to run the simulation
async def run_evaluation():
    meter = RAGMeter(
        questions=["What is AI?", "Explain neural networks."],
        ground_truth=["Artificial Intelligence refers to...", "A neural network is..."],
        llm_answers=["Artificial Intelligence is...", "Neural networks are..."],
        retrieved_chunks=[
            ["Some context for AI -1", "Some context for AI -2"],
            ["context for neural networks1", "context for neural networks2"],
        ],
        frameworks=[
            "ragas",   # Specify the RAG frameworks to be evaluated
            "arize",
            "deepeval",
            "trulens",
        ],
    )
    # Perform the evaluation asynchronously
    return await meter.multi_evaluation()

# Run the simulation and get the evaluation results
respond = asyncio.run(run_evaluation())

# Print the response or handle the results
print(respond)
```

### Explanation

1. **Setting Up the Environment**: The `OPENAI_API_KEY` environment variable is set to allow communication with the OpenAI API for evaluating responses.

2. **Initializing the `RAGMeter` Class**: The `RAGMeter` class is initialized with:
   - `questions`: A list of questions for evaluation.
   - `ground_truth`: The expected answers or ground truth corresponding to each question.
   - `llm_answers`: The answers generated by your LLM for each question.
   - `retrieved_chunks`: Context chunks that were retrieved during the retrieval process for each question.
   - `frameworks`: A list of evaluation frameworks to use, such as `ragas`, `arize`, `deepeval`, and `trulens`.

3. **Asynchronous Evaluation**: The evaluation is performed asynchronously using the `multi_evaluation` method, which returns the results of the evaluation.

4. **Running the Script**: The `asyncio.run(run_sim())` line executes the async function, and the evaluation results are printed at the end.

### Output

```json
{
  "questions": [
    {
      "question": "What is AI?",
      "expected_answer": "Artificial Intelligence refers to...",
      "generated_answer": "Artificial Intelligence is...",
      "retrieved_contexts": [
        "Some context for AI -1",
        "Some context for AI -2"
      ]
    },
    {
      "question": "Explain neural networks.",
      "expected_answer": "A neural network is...",
      "generated_answer": "Neural networks are...",
      "retrieved_contexts": [
        "context for neural networks1",
        "context for neural networks2"
      ]
    }
  ],
  "frameworks": [
    {
      "name": "ragas",
      "retrieval": {
        "context_recall": 0.0,
        "context_precision": 0.0,
        "answer_correctness": 0.686,
        "semantic_similarity": 0.96
      },
      "generation": {
        "answer_relevancy": 0.0,
        "faithfulness": 0.0
      }
    },
    {
      "name": "arize",
      "retrieval": {},
      "generation": {
        "hallucination_eval": 0.5,
        "qa_correctness_eval": 1.0,
        "toxicity_eval": 0.0
      }
    },
    {
      "name": "deepeval",
      "retrieval": {
        "context_relevancy": 0.0,
        "context_precision": 0.0,
        "context_recall": 0.5
      },
      "generation": {
        "answer_precision": 1.0,
        "faithfulness": 1.0
      }
    },
    {
      "name": "trulens",
      "retrieval": {},
      "generation": {
        "honest_score": 0.5,
        "harmless_score": 0.0,
        "helpful_score": 0.5
      }
    }
  ]
}
```

### Conclusion

This example provides a basic way to perform RAG (Retrieval-Augmented Generation) evaluations using different frameworks with the `rag_meter` package. For further customization, refer to the documentation or modify the parameters according to your use case.

---

This gives users a comprehensive and clear guide on how to set up and use your package. Let me know if you'd like to add or modify anything!
